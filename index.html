<style type="text/css">
	body {
		/* font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;  */
		font-family: "Noto Sans";
		font-weight: 300;
		font-size: 18px;
		margin-left: auto;
		margin-right: auto;
		width: 1200px;
	}

	h1 {
		font-size: 32px;
		font-weight: 300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 0px solid black;
		border-radius: 0px;
		-moz-border-radius: 0px;
		-webkit-border-radius: 0px;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	a:link,
	a:visited {
		color: #1367a7;
		text-decoration: none;
	}

	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35),
			/* The top layer shadow */
			5px 5px 0 0px #fff,
			/* The second layer */
			5px 5px 1px 1px rgba(0, 0, 0, 0.35),
			/* The second layer shadow */
			10px 10px 0 0px #fff,
			/* The third layer */
			10px 10px 1px 1px rgba(0, 0, 0, 0.35),
			/* The third layer shadow */
			15px 15px 0 0px #fff,
			/* The fourth layer */
			15px 15px 1px 1px rgba(0, 0, 0, 0.35),
			/* The fourth layer shadow */
			20px 20px 0 0px #fff,
			/* The fifth layer */
			20px 20px 1px 1px rgba(0, 0, 0, 0.35),
			/* The fifth layer shadow */
			25px 25px 0 0px #fff,
			/* The fifth layer */
			25px 25px 1px 1px rgba(0, 0, 0, 0.35);
		/* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35);
		/* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35),
			/* The top layer shadow */
			5px 5px 0 0px #fff,
			/* The second layer */
			5px 5px 1px 1px rgba(0, 0, 0, 0.35),
			/* The second layer shadow */
			10px 10px 0 0px #fff,
			/* The third layer */
			10px 10px 1px 1px rgba(0, 0, 0, 0.35);
		/* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}

	hr {
		border: 0;
		height: 1.5px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 1.0), rgba(0, 0, 0, 0));
		margin: 70px 0;
	}

	/* emjay added (start) */
	.checkmark-list {
		list-style-type: none;
		/* Remove default bullets */
		font-size: 17px;
		padding-left: 0;
		/* Remove default padding */
		margin-left: 0;
		/* Remove default margin */
	}

	.checkmark-list>li:before {
		content: '\2713';
		/* Unicode character for checkmark */
		margin-right: 8px;
		color: black;
		/* Optional: change checkmark color */
	}

	.checkmark-list ul {
		padding-left: 0;
		/* Remove default padding from nested lists */
		margin-left: 40px;
		/* Indent nested lists */
		list-style-type: none;
		/* Remove bullets from nested lists */
	}

	.checkmark-list ul>li:before {
		content: '\2713';
		/* Unicode character for checkmark */
		margin-right: 8px;
		color: black;
		/* Optional: change checkmark color for nested items */
	}

	.spaced-heading {
		margin-top: 20px;
		/* Adjust the value to increase or decrease space */
	}

	.figure-cell {
		padding-bottom: 40px;
		/* Adjust the value to increase or decrease space */
	}

	/* emjay added (end) */

	.icon {
		display: inline-block;
		width: 36px;
		/* Adjust as needed */
		height: 36px;
		/* Adjust as needed */
		background-image: url('./resources/Favicon.ico');
		background-size: contain;
		/* Ensure the SVG fits within the span */
		background-repeat: no-repeat;
		/* Prevent tiling */
		vertical-align: middle;
		/* Align with the text */
	}

	.noto-sans-<uniquifier> {
		font-family: "Noto Sans", sans-serif;
		font-optical-sizing: auto;
		font-weight: <weight>;
		font-style: normal;
		font-variation-settings:
			"wdth" 100;
	}

	.link-btn{
		background-color: #222;
		padding: 3px 20px;
		border-radius: 50px;
		margin-top: 10px;
		color: white;
	}
	.link-btn:visited{
		color: white;
	}
</style>

<html>

<head>
	<title>VEGS: View Extrapolation of Urban Scenes in 3D Gaussian Splatting using Learned Priors</title>
	<meta property="og:image" content="./resources/vegs_icon.png" />
	<!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title"
		content="VEGS: View Extrapolation of Urban Scenes in 3D Gaussian Splatting using Learned Priors" />
	<meta property="og:description" content="Neural rendering-based urban scene reconstruction methods commonly rely on images collected from driving vehicles with cameras facing and moving forward. 
											 Although these methods can successfully synthesize from views similar to training camera trajectory, directing the novel view outside the training camera distribution does not guarantee on-par performance. 
											 In this paper, we tackle the Extrapolated View Synthesis (EVS) problem by evaluating the reconstructions on views such as looking left, right or downwards with respect to training camera distributions. 
											 To improve rendering quality for EVS, we initialize our model by constructing dense LiDAR map, and propose to leverage prior scene knowledge such as surface normal estimator and large-scale diffusion model. 
											 Qualitative and quantitative comparisons demonstrate the effectiveness of our methods on EVS. 
											 To the best of our knowledge, we are the first to address the EVS problem in urban scene reconstruction.
" />

	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,100..900;1,100..900&display=swap"
		rel="stylesheet">
	<link rel="icon" href="./resources/Favicon.ico">
	<link rel="stylesheet" href="./definitive-image-comparison-slider-master/src/dics.css">
	<script src="./definitive-image-comparison-slider-master/src/dics.js"></script>

	<link rel="stylesheet" href="./definitive-image-comparison-slider-master/src/dics.css">
	<script src="./definitive-image-comparison-slider-master/src/dics.js"></script>


	<script>
		document.addEventListener('DOMContentLoaded', domReady);

		function domReady() {

			var b = document.querySelectorAll('.b-dics');
			b.forEach(element =>
				new Dics({
					container: element,
					textPosition: 'top'
				})
			);

		}
	</script>

</head>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<body>
	<br>
	<center>

		<table align=center width=1200px>
			<table align=center width=1200px>
				<tr>
					<td align=center width=300px>
						<center>
<!-- 							<span class="icon"></span> <!-- Add this line to include the icon -->
							<span style="font-size:39px"> <b>VEGS : View Extrapolation of Urban Scenes in 3D Gaussian
								Splatting using Learned Priors</b></span>
						</center>
					</td>
				</tr>
				<tr>
					<td align=center width=300px>
						<center>
							<span style="font-size:24px">ECCV 24</span>
						</center>
					</td>
				</tr>
			</table>
		</table>

		<br>

		<table align=center width=1200px>
			<table align=center width=900px>
				<tr>
					<td align=center width=300px>
						<center>
							<span style="font-size:24px"><a href="https://deepshwang.github.io/">Sungwon
									Hwang*</a></span>
							<br>
							<span style="font-size:14px">KAIST</span>
						</center>
					</td>
					<td align=center width=300px>
						<center>
							<span style="font-size:24px"><a href="https://vegs3d.github.io">Min-Jung Kim*</a></span>
							<br>
							<span style="font-size:14px">KAIST</span>
						</center>
					</td>
					<td align=center width=300px>
						<center>
							<span style="font-size:24px"><a href="https://keh0t0.github.io/">Taewoong Kang</a></span>
							<br>
							<span style="font-size:14px">KAIST</span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=600px>
				<tr>
					<td align=center width=300px>
						<center>
							<span style="font-size:24px"><a href="https://vegs3d.github.io">Jayeon Kang</a></span>
							<br>
							<span style="font-size:14px">Ghent University</span>
						</center>
					</td>
					<td align=center width=300px>
						<center>
							<span style="font-size:24px"><a href="https://sites.google.com/site/jaegulchoo">Jaegul
									Choo</a></span>
							<br>
							<span style="font-size:14px">KAIST</span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=600px>
				<tr>
					<td align=center width=600px>
						<center>
							<span style="font-size:14px">(*: equal contribution)</span>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<center>
		<table>
			<tr style="height: 70px;">	
				<td align=center> <span style="font-size:18pt">
						<center>
							<a class="link-btn" href="https://arxiv.org/abs/2103.03231">ArXiv</a>
						</center>
				</td>
				<td align=center> <span style="font-size:18pt">
						<center>
							<a class="link-btn" href="https://youtu.be/u9HqKGqvJhQ?t=5843">Code</a>
						</center>
				</td>
			</tr>
		</table>
	</center>
	<br>
	<center>
		<table align=center width=1200px>
			<tr>
				<td width=1200px >
					<center>
						<video autoplay muted loop>
							<source src="./resources/videos/final_evs_demo_v4.mp4" type="video/mp4">
							Your browser does not support the video tag.
						</video>
					</center>
				</td>
			</tr>
		</table>
	</center>
	<center>
		<table align=center width=1200px>
			<tr>
				<td align=center width=1200px style="height: 40px">
					<span style="font-size:16px;"><b>Our method aligns and flattens Gaussian covariances to scene
								surfaces estimated from monocular normal prediction network</b></span>
				</td>
			</tr>
		</table>
	</center>

	<!-- <center>
		<table align=center width=1200px>
			<tr>
				<td width=1200px>
					<center>
						<video controls autoplay muted loop>
							<source src="./resources/videos/3_render_0_0_0_less_small.mp4" type="video/mp4">
							Your browser does not support the video tag.
						  </video>
					</center>
				</td>
			</tr>
			<br>
			<tr>
				<td>
					<center>
					<span style="font-size:15px"><b>Our method jointly reconstructs static scene with dynamic object such as cars, which can then be relocated arbitrarily</b></span>
					</center>
				</td>
			</tr>
		</table>
	</center> -->

	<br>
	<center>
		<table align=center width=1200px>
			<tr>
				<td width=600px>
					<center>
						<video  autoplay muted loop>
							<source src="./resources/videos/3_render_0_0_0_dynamic_small.mp4" type="video/mp4">
							Your browser does not support the video tag.
						</video>
					</center>
				</td>
				<td width=600px>
					<center>
						<video autoplay muted loop>
							<source src="./resources/videos/5_render_0_0_0_small.mp4" type="video/mp4">
							Your browser does not support the video tag.
						</video>
					</center>
				</td>
			</tr>

			<tr>
				<td width=600px>
					<center>
						<video autoplay muted loop>
							<source src="./resources/videos/render_0_0_0_small.mp4" type="video/mp4">
							Your browser does not support the video tag.
						</video>
					</center>
				</td>
				<td width=600px>
					<center>
						<video autoplay muted loop>
							<source src="./resources/videos/2_render_0_0_0_small.mp4" type="video/mp4">
							Your browser does not support the video tag.
						</video>
					</center>
				</td>
			</tr>
		</table>
	</center>
	<center>
		<table align=center width=1200px>
			<tr>
				<td align=center width=1200px style="height: 40px">
					<span style="font-size:16px"><b>Our method jointly reconstructs static scene with dynamic objects
							such as cars.</b></span>
				</td>
			</tr>
		</table>
	</center>


	<br>
	<br>
	<table align=center width=1200px>
		<center>
			<h1>Paper Summary at a Glance</h1>
		</center>
		<hr>
		<h2 style="color:dimgrey;">Problem Statement</h2>

		<!-- </table> -->
		<!-- <br> -->
		<!-- <table align=center width=1200px> -->
		<!-- <center>
			<h1>Overall Pipeline</h1>
		</center> -->
		<tr>
			<td>
				<!-- <center> -->
			<td>
				<!-- <br> -->
				<!-- <ul class="checkmark-list">
					<li>Previous urban scene reconstruction methods commonly rely on images collected from driving
						vehicles with cameras facing and moving forward.</li>
					<li>These methods can successfully synthesize from views similar to training camera trajectory.</li>
					<li>However, directing the novel view outside the training camera distribution does not guarantee
						on-par performance.</li>
					<li>We tackle the Extrapolated View Synthesis (EVS) problem on views such as looking left, right or
						downwards with respect to train distributions.</li>
					<li>(a) Illustration of Extrapolated View Synthesis (EVS) problem in urban scenes reconstructed with
						forward-facing cameras. </li>
					<li>(b) Qualitative comparison on EVS to baselines. Rendering quality on conventional test cameras
						is not necessarily preserved on EVS.</li>
				</ul> -->
				<!-- <center>
					<span style="font-size:16px"><b>We tackle the Extrapolated View Synthesis (EVS) problem on views such as looking left, right or downwards from train camera distributions.</b></span>
				</center> -->
				<!-- <ul class="checkmark-list">
					<li>We tackle the Extrapolated View Synthesis (EVS) problem on views such as <b>looking left, right or downwards from train camera distributions.</b></li>
				</ul> -->
				We tackle the Extrapolated View Synthesis (EVS) problem on views such as <b>looking left, right or downwards from train camera distributions.</b></li>
				
				<!-- Neural rendering-based urban scene reconstruction methods commonly rely on images collected from driving vehicles with cameras facing and moving forward.
					Although these methods can successfully synthesize from views similar to training camera trajectory,
					directing the novel view outside the training camera distribution does not guarantee on-par performance.
					In this paper, we tackle the Extrapolated View Synthesis (EVS) problem by evaluating the reconstructions
					on views such as looking left, right or downwards with respect to training camera distributions.					
					(a) Illustration of Extrapolated View Synthesis (EVS) problem in urban scenes
					reconstructed with forward-facing cameras. In contrast to conventional test cameras
					similar to training camera poses, we evaluate view synthesis on cameras distant from
					training camera distribution. (b) Qualitative comparison on EVS to baselines. Render-
					ing quality on conventional test cameras is not necessarily preserved on EVS. -->
			</td>
			<!-- </center> -->


			</td>
		</tr>
		<tr>
			<td>
				<br>
			</td>
		</tr>
		<tr>
			<!-- <td align=center width=1200px> -->
			<td>
				<center>
			<td><img class="round" style="width:1200px" src="./resources/teaser_projectpage_v2.png" /></td>
			</center>
			</td>
		</tr>
	</table>

	<br>
	<table align=center width=1200px>
		<!-- <center> -->
		<h2 style="color:dimgrey;">Overall Pipeline</h2>
		<!-- </center> -->
		
		<!-- </table> -->
		<!-- <br> -->
		<!-- <table align=center width=1200px> -->
		<!-- <center>
			<h1>Overall Pipeline</h1>
		</center> -->

		<tr>
			<td>
				<center>
			<td>
				We initialize gaussian means using dense <b>LiDAR map</b> and <b>point cloud from SfM</b>.
				We leveraged prior scene knowledge, such as <b>surface normal estimation</b>
						and <b>large-scale diffusion models</b>, to improve rendering quality for EVS.
				<!-- <ul class="checkmark-list"> -->
					<!-- <li>We initialize gaussian means using dense <b>LiDAR map</b> and <b>point cloud from SfM</b>.</li> -->
					<!-- <li>To improve rendering quality for EVS, prior scene knowledge, such as <b>surface normal estimation</b>
						and <b>large-scale diffusion models</b>, is leveraged.</li> -->
					<!-- <li>We leveraged prior scene knowledge, such as <b>surface normal estimation</b>
						and <b>large-scale diffusion models</b>, to improve rendering quality for EVS.</li> -->
					<!-- <li>To utilize surface normal prior, we propose Covariance Guidance Loss.
						<ul class="checkmark-list">
							<li>\(\mathcal{L}_{\text{cov}} = \lambda_{\text{axis}} \mathcal{L}_{\text{axis}} + (1 -
								\lambda_{\text{axis}}) \mathcal{L}_{\text{scale}}\)</li>
							<li>\(L_{axis}\) is minimized when any of the three covariance axes aligns with the normal
								vector. </li>
							<li>\(L_{scale}\) is minimized when scale of the axis aligned to normal is minimized,
								inducing the covariance to mimic an underlying surface. </li>							
						</ul>
					</li>
					<li>To utilize large-scale diffusion model, we exploit the score distillation.
						<ul class="" checkmark-list">
							<li>\(\nabla_{M} \mathcal{L}_{\text{score}} = -
								\textbf{s}_{\theta}(\hat{\mathcal{I}}_{\tau}, \tau) \approx \nabla_{\textbf{x}}
								\text{log} p(\textbf{x})\)</li>
							<li>\(\hat{\mathcal{I}_{\tau}} = \sqrt{\bar{\alpha}_{\tau}} \hat{\mathcal{I}} + (1 -
								\bar{\alpha}_{\tau})\epsilon\), where \(\hat{\mathcal{I}}\) is a rendering from our
								model \(M\) on EVS.</li>
						</ul>
					</li>
					<li>To the best of our knowledge, we are the first to address the EVS problem in urban scene
						reconstruction.</li> -->
				<!-- </ul> -->

				<!-- To improve rendering quality for EVS, we initialize our model by constructing dense LiDAR map, and
					propose to leverage prior scene knowledge such as surface normal estimator and large-scale diffusion
					model.
					\(\nabla_{M} \mathcal{L}_{\text{score}} = - \textbf{s}_{\theta}(\hat{\mathcal{I}}_{\tau}, \tau) \approx \nabla_{\textbf{x}} \text{log} p(\textbf{x})\)
					where \(\hat{\mathcal{I}_{\tau}} = \sqrt{\bar{\alpha}_{\tau}} \hat{\mathcal{I}} + (1 - \bar{\alpha}_{\tau})\epsilon\) and \(\hat{\mathcal{I}}\) is a rendering from our model \(M\) on EVS. -->
				<!-- Qualitative and quantitative comparisons demonstrate the effectiveness of our methods on EVS. -->

			</td>
			</center>
			</td>
		</tr>
		<tr>
			<td>
				<br>
			</td>
		</tr>
		<tr>
			<!-- <td align=center width=1200px> -->
			<td>
				<center>
			<td><img class="round" style="width:1200px" src="./resources/pipeline_v3.png" /></td>
			</center>
			</td>
		</tr>
	</table>
	<br>
	<br>
	<table align=center width=1200px>
		<!-- <center><h1><b>The Lazy Covariance Optimization Problem</b></h1></center> -->
		<center>
			<h1>Covariance Guidance with Surface Normal Prior</h1>
			<hr>
		</center>

		<tr>
			<td>
			<td class="spaced-heading">
				<h2 style="color:dimgrey;">The Lazy Covariance Optimization (LCO) Problem</h2>
			</td>
			</td>
		</tr>
		<tr> </tr>
		<tr>
			<td>
				<center>
			<td>
				<ul class="checkmark-list">
					<!-- <li>The shape and orientation of learned covariances tend to over-fit to a certain viewing angle.</li> -->
					<!-- <li>LCO problem refers to the case where the covariance is trained to cover the the frustum of a training pixel with a minimal optimization effort.</li>
					<li>As a result, these covariances are prone to produce unwanted cavity on an underlying scene
						surface.</li> -->
					LCO problem refers to the case where the covariance is trained to cover the the frustum of a training pixel with a minimal optimization effort.
					As a result, these covariances are prone to produce unwanted cavity on an underlying scene surface.
					<!-- <li>The right-most ellipsoid in (a) shows such a case.</li> -->
				</ul>
			</td>
			</center>
			</td>
		</tr>
		<tr>
			<td>
				<center>
			<td class="figure-cell"><img class="round" style="width:1200px"
				src="./resources/normal_prior_crop.png" /></td>
					<!-- src="./resources/normal_prior_merge_v6.1.png" /></td> -->
			</center>
			</td>
		</tr>
		<tr>
			<td>
			<td>
				<h2 style="color:dimgrey;">Covariance Guidance Loss</h2>
			</td>
			</td>
		</tr>
		<tr>
			<td>
				<center>
			<td>
				<!-- <ul class="checkmark-list"> -->
					Our key idea is to guide the orientation and shape of covariances to make them behave like the
						underlying scene surface. Specifically, we propose \(\mathcal{L}_{cov}\) =\(\mathcal{L}_{axis}\) + \(\mathcal{L}_{scale}\), where
						\(\mathcal{L}_{axis}\) aligns covariance axes to a surface normal vector and 
							\(\mathcal{L}_{scale}\) minimizes the scale along the covariance axis aligned with surface normal

					<!-- <li>(b) Visualizing \(L_{axis}\) for different alignment between normal and covariances. \(L_{axis}\) is minimized when an axis aligns with the normal.</li> -->
					<!-- \(\mathcal{L}_{\text{scale}} = \sum\limits_{i\in \{\text{0,1,2}\}} {\left|\tilde{\mathbf{S}}[i]\left(\tilde{\mathbf{Q}}[:, i] \cdot G(\mathcal{I}) \right)\right|}/3,\) -->
					<!-- \(\tilde{\textbf{q}} = \prod_{j \in \mathcal{N}} \mathcal{S}\left( \textbf{q}_{I}, \textbf{q}_j, w_{j} \right), \ \ \ w_{j} = \alpha_{j} \prod_{l=1}^{j-1} (1 - \alpha_{l})\) -->
					<!-- \(\mathcal{L}_{\text{axis}} = \sum\limits_{i\in \{\text{0,1,2}\}} {|\tilde{\mathbf{Q}}[:, i] \cdot G(\mathcal{I})|}/3,\) -->
				<!-- </ul> -->

				<!-- The shape and orientation of learned covariances tend to over-fit to a certain viewing angle,
					which we hypothesize that the covariance is trained to cover the the frustum of a training pixel with a
					minimal optimization effort.
					As a result, these covariances are prone to produce unwanted cavity on an underlying scene surface,
					which is revealed when viewed from unobserved angles.
					Our key idea is to guide the orientation and shape of covariances to make them behave like the
					underlying scene surface.
					
					(b) Visualizing \(L_{axis}\) for different alignment between normal and covariances.
					\(L_{axis}\) is minimized when an axis aligns with the normal. See supplements for detailed derivation. -->
			</td>
			</center>
			</td>
		</tr>
	</table>

	<!-- <hr> -->
	<!-- <table align=center width=1200px>
		<center><h1>Real-Time View Synthesis</h1></center>
		<h2 class="grey-heading_nerf">Relighting</h2>
		<tr>
			<td>
				Due to our novel depth oracle sampling scheme, DONeRF achieves quality similar to <a href="https://www.matthewtancik.com/nerf">NeRF</a>, which uses a total of 256 samples.
				At only 4 samples (comparison to NeRF below), DONeRF achieves a speedup of 20x-48x at the same quality. 
        Click / Drag the Sliders to compare various outputs between DONeRF, NeRF and Ground Truth Blender renderings.
			</td>
		</tr>
	</table>
	<br> -->

	<br>
	<!-- <h2 style="color:dimgrey;">Qualitative Ablation Results on \(L_{cov}\)</h2> -->
	<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
			<td>
				<div class="b-dics" style="width: 400px">
					<!-- 44 -->
					<!-- <img src="./resources/1_diffusion_only_normal_ablation.png" width=400px  alt="Diffusion Only"> -->
					<img src="./resources/1_free_diffusion_only_normal_ablation.png" width=400px alt="w/o Lcov">
					<img src="./resources/1_free_ours_normal_ablation.png" width=400px alt="VEGS">
				</div>
			</td>
			</center>
			</td>
			<!-- </tr>
		<tr> -->
			<td align=center width=400px>
				<center>
			<td>
				<div class="b-dics" style="width: 400px">
					<!-- 39 -->
					<img src="./resources/2_free_diffonly_0000008926_Rx0_Rz-60_tz0.png" width=400px alt="w/o Lcov">
					<img src="./resources/2_free_ours_0000008926_Rx0_Rz-60_tz0.png" width=400px alt="VEGS">
				</div>
				</div>
			</td>
			</center>
			</td>
			<!-- </tr>
		<tr> -->
			<td align=center width=400px>
				<center>
			<td>
				<div class="b-dics" style="width: 400px">
					<!-- 57 -->
					<img src="./resources/3_free_diffonly_0000009646_Rx-16_Rz0_tz1.6.png" width=400px alt="w/o Lcov">
					<img src="./resources/3_free_ours_0000009646_Rx-16_Rz0_tz1.6.png" width=400px alt="VEGS">
				</div>
			</td>
			</center>
			</td>
		</tr>
	</table>

	<!--  -->
	<br>
	<br>

	<table align=center width=1200px>
		<center>
			<h1>Score Distillation from Large-scale Diffusion Model</h1>
			<hr>
		</center>
		<tr>
			<td>
				Noise (score) predicted from a diffusion model \( \textbf{s}_{\theta} \) is proportional to the
				log-gradient of a prior distribution \( p(\textbf{x}) \), or \( \textbf{s}_{\theta}(\textbf{x}_{\tau},
				\tau) \approx - \nabla_{\textbf{x}} \text{log} p(\textbf{x}) \).
				Thus, optimizing \( \textbf{x}_{\tau} \) to yield smaller score pushes \( \textbf{x} \) to our prior
				distribution \( p(\cdot) \). We model our prior distribution using <a
					href="https://github.com/CompVis/stable-diffusion">Stable Diffusion</a> fine-tuned with <a
					href="https://arxiv.org/abs/2106.09685">LoRA</a>.
			</td>
		</tr>
	</table>

	<table align=center width=1200px>

		<tr>
			<td align=center width=400px>
				<center>
			<td>
				<div class="b-dics" style="width: 400px">
					<!-- 44 -->
					<img src="./resources/diffusion_baseline_1.png" width=400px alt="w.o/ score loss">
					<img src="./resources/diffusion_ours_1.png" width=400px alt="w/ score loss">
				</div>
			</td>
			</center>
			</td>
			<td align=center width=400px>
				<center>
			<td>
				<div class="b-dics" style="width: 400px">
					<!-- 39 -->
					<img src="./resources/diffusion_baseline_3.png" width=400px alt="w.o/ score loss">
					<img src="./resources/diffusion_ours_3.png" width=400px alt="w/ score loss">
				</div>
				</div>
			</td>
			</center>
			</td>
			<td align=center width=400px>
				<center>
			<td>
				<div class="b-dics" style="width: 400px">
					<!-- 57 -->
					<img src="./resources/diffusion_baseline_2.png" width=400px alt="w.o/ score loss">
					<img src="./resources/diffusion_ours_2.png" width=400px alt="w/ score loss">
				</div>
			</td>
			<!--  -->
		</tr>
	</table>
	<br>
	<br>

	<table align=center width=1200px>
				<center>
					<h1>Comparison to Baseline</h1>
					<hr>
				</center>
				<tr>
					<td>
						<!-- <ul class="checkmark-list">
							<li>Qualitative comparison on KITTI-360 for extrapolated view synthesis.</li>
							<li>EVS-D and EVS-LR refers to extrapolated views facing downwards and left/right, respectively.</li>
						</ul> -->
						EVS-D and EVS-LR refers to extrapolated views facing downwards and left/right, respectively.
						
					</td>
				</tr>
			</table>
			<br>
		</tr>
	</table>
	<table align=center width=1200px>

		<tr>
			<td align=center width=400px>
				<center>
			<td>
				<div class="b-dics" style="width: 400px">
					<!-- 44 -->
					<img src="./resources/cp_down1_mars_0000004230_Rx_3A -12_7C Rz_3A 0 _7C tz_3A 0.12.png" width=400px
						alt="MARS">
					<img src="./resources/cp_down1_bn_00325_frame_00004230.0.png" width=400px alt="BlockNeRF++">
					<img src="./resources/cp_down1_3dgs_0000004230_Rx-12_Rz0_tz1.2000000000000002.png" width=400px
						alt="3DGS">
					<img src="./resources/cp_down1_ours_0000004230_Rx-12_Rz0_tz1.2000000000000002.png" width=400px
						alt="VEGS">
				</div>
			</td>
			</center>
			</td>

			<td align=center width=400px>
				<center>
			<td>
				<div class="b-dics" style="width: 400px">
					<!-- 44 -->
					<img src="./resources/cp_down2_mars_0000003622_Rx_3A -16_7C Rz_3A 0 _7C tz_3A 0.16.png" width=400px
						alt="MARS">
					<img src="./resources/cp_down2_01957_frame_00003622.0.png" width=400px alt="BlockNeRF++">
					<img src="./resources/cp_down2_3dgs_2013_05_28_drive_0006_sync_static_0000003251_0000003634-0000003622_Rx-16_Rz0_tz1.6.png"
						width=400px alt="3DGS">
					<img src="./resources/cp_down2_ours_2013_05_28_drive_0006_sync_static_0000003251_0000003634-0000003622_Rx-16_Rz0_tz1.6.png"
						width=400px alt="VEGS">
				</div>
			</td>
			</center>
			</td>

			<td align=center width=400px>
				<center>
			<td>
				<div class="b-dics" style="width: 400px">
					<!-- 39 -->
					<img src="./resources/cp_lr2_mars_0000003497_Rx_3A 0_7C Rz_3A 60 _7C tz_3A 0.png" width=400px
						alt="MARS">
					<img src="./resources/cp_lr2_bn_00312_frame_00003497.0.png" width=400px alt="BlockNeRF++">
					<img src="./resources/cp_lr2_splat_0000003497_Rx0_Rz-60_tz0_440.png" width=400px alt="3DGS">
					<img src="./resources/cp_lr2_ours_0000003497_Rx0_Rz-60_tz0.png" width=400px alt="VEGS">
				</div>
				</div>
			</td>
			</center>
			</td>

			<!-- <td align=center width=400px>
				<center>
			<td>
				<div class="b-dics" style="width: 400px">					
					<img src="./resources/cp_lr3_mars_0000002173_Rx_3A 0_7C Rz_3A 30 _7C tz_3A 0.png" width=400px
						alt="MARS">
					<img src="./resources/cp_lr3_00310_frame_00002173.0.png" width=400px alt="BlockNeRF++">
					<img src="./resources/cp_lr3_3dgs_2013_05_28_drive_0009_syncstatic0000002117_0000002353_0000002173_Rx0_Rz-30_tz0.png" width=400px
						alt="3DGS">
					<img src="./resources/cp_lr3_ours_2013_05_28_drive_0009_syncstatic0000002117_0000002353_0000002173_Rx0_Rz-30_tz0.png" width=400px
						alt="VEGS">
				</div>
			</td>
			</center>
			</td> -->
		</tr>
		<tr>
			<td align=center width=400px>
				<center>
			<th>
				EVS-D
			</th>
			</center>
			</td>
			<td align=center width=400px>
				<center>
			<th>
				EVS-D
			</th>
			</center>
			</td>
			<td align=center width=400px>
				<center>
			<th>
				EVS-LR
			</th>
			</center>
			</td>
		</tr>
	</table>
	<br>

	<!--	
	<table align=center width=1200px>
		<br>
		<tr>
			<center>
				<span style="font-size:28px"><a href=''>[Slides]</a>
				</span>
			</center>
		</tr>
	</table>

	<hr>

	<center><h1>Code</h1></center>

	<table align=center width=420px>
		<center>
			<tr>
				<td>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:450px" src="./resources/method_diagram.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=1200px>
		<center>
			<tr>
				<td>
					Short description if wanted
				</td>
			</tr>
		</center>
	</table>
	
	<table align=center width=1200px>
		<br>
		<tr><center>
			<span style="font-size:28px">&nbsp;<a href='https://github.com/richzhang/webpage-template'>[GitHub]</a>
			</center>
		</span>
	</table>
	-->
	<br>
	<!--
  <img-comparison-slider>
    <img slot="before" src="./resources/method_diagram.png" />
    <img slot="after" src="./resources/teaser.png" />
  </img-comparison-slider>
-->
</body>

</html>
