<style type="text/css">
	body {
		/* font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;  */
		font-family: "Noto Sans"; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1200px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 0px solid black;
		border-radius: 0px ;
		-moz-border-radius: 0px ;
		-webkit-border-radius: 0px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1.5px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 1.0), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>VEGS: View Extrapolation of Urban Scenes in 3D Gaussian Splatting using Learned Priors</title>
	<meta property="og:image" content="./resources/teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="VEGS: View Extrapolation of Urban Scenes in 3D Gaussian Splatting using Learned Priors" />
	<meta property="og:description" content="Neural rendering-based urban scene reconstruction methods commonly rely on images collected from driving vehicles with cameras facing and moving forward. 
											 Although these methods can successfully synthesize from views similar to training camera trajectory, directing the novel view outside the training camera distribution does not guarantee on-par performance. 
											 In this paper, we tackle the Extrapolated View Synthesis (EVS) problem by evaluating the reconstructions on views such as looking left, right or downwards with respect to training camera distributions. 
											 To improve rendering quality for EVS, we initialize our model by constructing dense LiDAR map, and propose to leverage prior scene knowledge such as surface normal estimator and large-scale diffusion model. 
											 Qualitative and quantitative comparisons demonstrate the effectiveness of our methods on EVS. 
											 To the best of our knowledge, we are the first to address the EVS problem in urban scene reconstruction.
" />

  
  <link rel="stylesheet" href="./definitive-image-comparison-slider-master/src/dics.css">
  <script src="./definitive-image-comparison-slider-master/src/dics.js"></script>

  
	<script>
    document.addEventListener('DOMContentLoaded', domReady);
    
    function domReady() {
    
      var b = document.querySelectorAll('.b-dics');
      b.forEach(element => 
        new Dics({
          container: element,
          textPosition: 'top'
        })
      );

    }
  
    

    
	</script>
  
</head>

<body>
	<br>
	<center>

		<table align=center width=1200px>
			<table align=center width=1200px>
				<tr>
					<td align=center width=300px>
						<center>
							<span style="font-size:36px">VEGS: View Extrapolation of Urban Scenes in 3D Gaussian Splatting using Learned Priors</span>
						</center>
					</td>
				</tr>
        <tr>
          <td align=center width=300px>
						<center>
							<span style="font-size:24px">ECCV 24</span>
						</center>
					</td>
        </tr>
			</table>
		</table>
		
		<br>
		
		<table align=center width=1200px>
			<table align=center width=900px>
				<tr>
					<td align=center width=300px>
						<center>
							<span style="font-size:24px"><a href="https://deepshwang.github.io/">Sungwon Hwang*</a></span>
							<br>
							<span style="font-size:14px">KAIST</span>
						</center>
					</td>
					<td align=center width=300px>
						<center>
							<span style="font-size:24px"><a href="https://vegs3d.github.io">Min-Jung Kim*</a></span>
							<br>
							<span style="font-size:14px">KAIST</span>
						</center>
					</td>
					<td align=center width=300px>
						<center>
							<span style="font-size:24px"><a href="https://keh0t0.github.io/">Taewoong Kang</a></span>
							<br>
							<span style="font-size:14px">KAIST</span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=600px>
				<tr>
					<td align=center width=300px>
						<center>
							<span style="font-size:24px"><a href="https://vegs3d.github.io">Jayeon Kang</a></span>
							<br>
							<span style="font-size:14px">Ghent University</span>
						</center>
					</td>
					<td align=center width=300px>
						<center>
							<span style="font-size:24px"><a href="https://sites.google.com/site/jaegulchoo">Jaegul Choo</a></span>
							<br>
							<span style="font-size:14px">KAIST</span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=600px>
				<tr>
					<td align=center width=600px>
						<center>
							<span style="font-size:14px">(*: equal contribution)</span>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<center>
		<table>
		  <tr>
			<td align=center> <span style="font-size:18pt">
			  <center>
				<a href="https://arxiv.org/abs/2103.03231">|  ArXiv  |</a>
			  </center>
			</td>
			<td align=center> <span style="font-size:18pt">
			  <center>
				<a href="https://youtu.be/u9HqKGqvJhQ?t=5843">|  Code  |</a>
			  </center>
			</td>
		  </tr>
		</table>
	</center>

	<center>
		<table align=center width=1200px>
			<tr>
				<td width=1200px>
					<center>
						<video controls autoplay muted loop>
							<source src="./resources/videos/final_evs_demo_v4.mp4" type="video/mp4">
							Your browser does not support the video tag.
						  </video>
					</center>
				</td>
			</tr>
			<br>
			<tr>
				<td>
					<center>
					<span style="font-size:16px"><b>Our method aligns and flattens Gaussian covariances to scene surfaces estimated from monocular normal prediction network</b></span>
					</center>
				</td>
			</tr>
		</table>
	</center>

	<!-- <center>
		<table align=center width=1200px>
			<tr>
				<td width=1200px>
					<center>
						<video controls autoplay muted loop>
							<source src="./resources/videos/3_render_0_0_0_less_small.mp4" type="video/mp4">
							Your browser does not support the video tag.
						  </video>
					</center>
				</td>
			</tr>
			<br>
			<tr>
				<td>
					<center>
					<span style="font-size:15px"><b>Our method jointly reconstructs static scene with dynamic object such as cars, which can then be relocated arbitrarily</b></span>
					</center>
				</td>
			</tr>
		</table>
	</center> -->

	<br>

	<center>
		<table align=center width=1200px>
			<tr>
				<td width=600px>
					<center>
						<video controls autoplay muted loop>
							<source src="./resources/videos/3_render_0_0_0_dynamic_small.mp4" type="video/mp4">
							Your browser does not support the video tag.
						  </video>
					</center>
				</td>
				<td width=600px>
					<center>
						<video controls autoplay muted loop>
							<source src="./resources/videos/5_render_0_0_0_small.mp4" type="video/mp4">
							Your browser does not support the video tag.
						  </video>
					</center>
				</td>
			</tr>

			<tr>
				<td width=600px>
					<center>
						<video controls autoplay muted loop>
							<source src="./resources/videos/render_0_0_0_small.mp4" type="video/mp4">
							Your browser does not support the video tag.
						  </video>
					</center>
				</td>
				<td width=600px>
					<center>
						<video controls autoplay muted loop>
							<source src="./resources/videos/2_render_0_0_0_small.mp4" type="video/mp4">
							Your browser does not support the video tag.
						  </video>
					</center>
				</td>
			</tr>
		</table>

		<table align=center width=1200px>
			<tr>
				<td>
					<span style="font-size:15px"><b>Our method jointly reconstructs static scene with dynamic object such as cars.</b></span>
				</td>
			</tr>
		</table>

	</center>	

	<!-- <center>
		<table align=center width=1200px>
			<tr>
				<td width=400px>
					<center>
						<video controls autoplay muted loop>
							<source src="./resources/videos/sanmiguel_donerf16_rotate_1.mp4" type="video/mp4">
							Your browser does not support the video tag.
						  </video>
					</center>
				</td>
				<td width=400px>
					<center>
						<video controls autoplay muted loop>
							<source src="./resources/videos/classroom_donerf16_pan_1.mp4" type="video/mp4">
							Your browser does not support the video tag.
						  </video>
					</center>
				</td>
				<td width=400px>
					<center>
						<video controls autoplay muted loop>
							<source src="./resources/videos/pavillon_donerf16_rotate_1.mp4" type="video/mp4">
							Your browser does not support the video tag.
						  </video>
					</center>
				</td>
			</tr>
		</table>
	</center> -->

	<br>
	<hr>


	<table align=center width=1200px>
		<center><h1>Pipeline</h1></center>
		<tr>
			<td align=center width=1200px>
				<center>
					<td><img class="round" style="width:1200px" src="./resources/pipeline_v3.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<br>
	<hr>
	<table align=center width=1200px>
		<center><h1>Real-Time View Synthesis</h1></center>
		<tr>
			<td>
				Due to our novel depth oracle sampling scheme, DONeRF achieves quality similar to <a href="https://www.matthewtancik.com/nerf">NeRF</a>, which uses a total of 256 samples.
				At only 4 samples (comparison to NeRF below), DONeRF achieves a speedup of 20x-48x at the same quality. 
        Click / Drag the Sliders to compare various outputs between DONeRF, NeRF and Ground Truth Blender renderings.
			</td>
		</tr>
	</table>
	<br>
  
  <br>
	<table align=center width=1200px>
		<tr>
			<td align=center width=400px>
				<center>
					<td>
            <div class="b-dics" style="width: 400px">
              <!-- 44 -->
              <img src="./resources/sanmiguel_donerf4.png" width=400px  alt="DONeRF">
              <img src="./resources/sanmiguel_gt.png" width=400px  alt="Target">
              <img src="./resources/sanmiguel_nerf.png" width=400px  alt="NeRF">
            </div>
					</td>
				</center>
			</td>
			<td align=center width=400px>
				<center>
					<td>
						<div class="b-dics" style="width: 400px">
              <!-- 39 -->
              <img src="./resources/classroom_donerf4.png" width=400px  alt="DONeRF">
              <img src="./resources/classroom_gt.png" width=400px  alt="Target">
              <img src="./resources/classroom_nerf.png" width=400px  alt="NeRF">
            </div>
						</div>
					</td>
				</center>
			</td>
			<td align=center width=400px>
				<center>
					<td>
						<div class="b-dics" style="width: 400px">
              <!-- 57 -->
              <img src="./resources/forest_donerf4.png" width=400px  alt="DONeRF">
              <img src="./resources/forest_gt.png" width=400px  alt="Target">
              <img src="./resources/forest_nerf.png" width=400px  alt="NeRF">
            </div>
					</td>
				</center>
			</td>
		</tr>
	</table>
  <br>
  <hr>

  <table align=center width=1200px>
	<center><h1>Depth Oracle Predicition</h1></center>
	<tr>
		<td>
			Our Depth Oracle predicts multiple potential sampling candidates along each ray by discretizing the space along rays and predicting sampling probabilities along rays.
			The 3 color channels encode the 3 highest probabilities along the ray - gray values illustrate that there is likely only a single surface that should be sampled, while colorful values indicate that samples need to be spread out in depth.
			Even a relatively coarse depth prediction is sufficient for DONeRF to place samples efficiently.
		</td>
	</tr>
</table>
<br>

  <table align=center width=1200px>
	
	<tr>
		<td align=center width=400px>
			<center>
				<td>
		<div class="b-dics" style="width: 400px">
		  <!-- 44 -->
		  <img src="./resources/sanmiguel_depth_donerf.png" width=400px  alt="DONeRF">
		  <img src="./resources/sanmiguel_depth_target.png" width=400px  alt="Target">
		</div>
				</td>
			</center>
		</td>
		<td align=center width=400px>
			<center>
				<td>
					<div class="b-dics" style="width: 400px">
		  <!-- 39 -->
		  <img src="./resources/classroom_depth_donerf.png" width=400px  alt="DONeRF">
		  <img src="./resources/classroom_depth_target.png" width=400px  alt="Target">
		</div>
					</div>
				</td>
			</center>
		</td>
		<td align=center width=400px>
			<center>
				<td>
					<div class="b-dics" style="width: 400px">
		  <!-- 57 -->
		  <img src="./resources/forest_depth_donerf.png" width=400px  alt="DONeRF">
		  <img src="./resources/forest_depth_target.png" width=400px  alt="Target">
		</div>
				</td>
			</center>
		</td>
	</tr>
</table>
<br>

<hr>

<table align=center width=1200px>
	<center><h1><a href="https://research.nvidia.com/publication/2020-07_FLIP">FLIP</a> Comparison</h1></center>
	<tr>
		<td>
			We use the <a href="https://research.nvidia.com/publication/2020-07_FLIP">FLIP</a> error estimator to produce error maps that model how likely humans would perceive errors when "flipping" between an image and the target output. 
			DONeRF shows similar or better results at significantly lower performance requirements.
		</td>
	</tr>
</table>
<br>

<table align=center width=1200px>
	<tr>
		<td align=center width=400px>
			<center>
				<td>
		<div class="b-dics" style="width: 400px">
		  <!-- 44 -->
		  <img src="./resources/sanmiguel_donerf_flip.png" width=400px  alt="DONeRF">
		  <img src="./resources/sanmiguel_nerf_flip.png" width=400px  alt="NeRF">
		</div>
				</td>
			</center>
		</td>
		<td align=center width=400px>
			<center>
				<td>
					<div class="b-dics" style="width: 400px">
		  <!-- 39 -->
		  <img src="./resources/classroom_donerf_flip.png" width=400px  alt="DONeRF">
		  <img src="./resources/classroom_nerf_flip.png" width=400px  alt="NeRF">
		</div>
					</div>
				</td>
			</center>
		</td>
		<td align=center width=400px>
			<center>
				<td>
					<div class="b-dics" style="width: 400px">
		  <!-- 57 -->
		  <img src="./resources/forest_donerf_flip.png" width=400px  alt="DONeRF">
		  <img src="./resources/forest_nerf_flip.png" width=400px  alt="NeRF">
		</div>
				</td>
			</center>
		</td>
	</tr>
</table>

<br>
<hr>
<table align=center width=1200px>
	<center><h1>Video Summary</h1></center>
	<tr>
		<td>
			<iframe width="1200" height="675" src="https://www.youtube.com/embed/6UE1dMUjN_E" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		</td>
	</tr>
</table>

<!--	
	<table align=center width=1200px>
		<br>
		<tr>
			<center>
				<span style="font-size:28px"><a href=''>[Slides]</a>
				</span>
			</center>
		</tr>
	</table>

	<hr>

	<center><h1>Code</h1></center>

	<table align=center width=420px>
		<center>
			<tr>
				<td>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:450px" src="./resources/method_diagram.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=1200px>
		<center>
			<tr>
				<td>
					Short description if wanted
				</td>
			</tr>
		</center>
	</table>
	
	<table align=center width=1200px>
		<br>
		<tr><center>
			<span style="font-size:28px">&nbsp;<a href='https://github.com/richzhang/webpage-template'>[GitHub]</a>
			</center>
		</span>
	</table>
	-->
	<br>
	<hr>
	<table align=center width=450px>
		<center><h1>Paper and Supplementary Material</h1></center>
		<tr>
			<td><a href="https://diglib.eg.org/handle/10.1111/cgf14340"><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:14pt">T. Neff, P. Stadlbauer, M. Parger, A. Kurz, J. H. Mueller, C. R. A. Chaitanya, A. Kaplanyan, M. Steinberger<br>
				<b>DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks</b><br>				
				(published in <a href="https://diglib.eg.org/handle/10.1111/cgf14340">Computer Graphics Forum (EGSR 2021)</a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>
  
<!--
  <img-comparison-slider>
    <img slot="before" src="./resources/method_diagram.png" />
    <img slot="after" src="./resources/teaser.png" />
  </img-comparison-slider>
-->
	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>



	<hr>
	<br>

	


	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

